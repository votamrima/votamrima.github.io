---
layout: single
title: "Node Maintenance Tasks"
subtitle: ""
date: 2025-03-16 08:15:00 +0100
background: '/image/01.jpg'
tags: ['kubernetes']
---

{% raw %}

This is the some note about the performing basic maintenance tasks in Kubernetes cluster. 

### Metrics Server

Metrics Server doesn't install on Kubernetes from the box. We need to install it seperately from the official github repository - https://github.com/kubernetes-sigs/metrics-server 

````bash
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
````

Once installation is finished, you can see that pod is running, but no container is up:

````bash
seymur@fedora40:~$ kubectl -n kube-system get pods
NAME                                            READY   STATUS    RESTARTS        AGE
.....
metrics-server-54bf7cdd6-8sr98                  0/1     Running   0               2m13s
````

Here you need to configure the Metric Server deployment to allow insecure tls. To solve this I edited the deployment:
````bash
kubectl -n kube-system edit deployments.apps metrics-server
````

and added ``--kubelet-insecure-tls`` parameter:

````bash
spec:
      containers:
      - args:
        - --kubelet-insecure-tls
````

After that the single container was run successfull.

To check if metrics server does what it should do, run the following command:

````bash
seymur@fedora40:~$ kubectl top pods
NAME                          CPU(cores)   MEMORY(bytes)   
mypod-kube-worker1.home.lab   0m           4Mi             
````

### Etcd Backup

Etcd is a database service of the Kubernetes
- etcd is a core Kubernetes service that contains all resources that have been created
- it is a static pod and is started by kubelet
- by losing etcd you lose all your cluster configuration

The backup of the etcd is able to proceed using etcdctl tool. This is the client of etcd database. Unfortunatelly, I couldn't download from repository, and I downloaded it from the official github repository to the first control node: 

````bash
wget https://github.com/etcd-io/etcd/releases/download/v3.5.19/etcd-v3.5.19-linux-amd64.tar.gz
````

By extracting the archive I used the utility easily. To test if etcdctl is able to communicate with server was run following commands:

````bash
[root@kube-master1 etcd-v3.5.19-linux-amd64]# ./etcdctl --endpoints=https://127.0.0.1:2379 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key member list
10d8c2fc2b2b053c, started, kube-master3.home.lab, https://192.168.11.73:2380, https://192.168.11.73:2379, false
373f7e005da9deca, started, kube-master2.home.lab, https://192.168.11.72:2380, https://192.168.11.72:2379, false
aa70f81cf7c7bc59, started, kube-master1.home.lab, https://192.168.11.71:2380, https://192.168.11.71:2379, false
````

In this example the etcdctl tool lists the active etcd servers in the cluster.
To run the utility is required to add following parameters to the command:
``--etcd-servers=https://127.0.0.1:2379``
``--cacert /etc/kubernetes/pki/etcd/ca.crt`` 
``--cert /etc/kubernetes/pki/etcd/server.crt`` 
``--key /etc/kubernetes/pki/etcd/server.key``

The values for the parameters are able to find by running ``ps -aux | grep etcd`` command at control node. Additionally, the certificates are stored in ``/etc/kubernetes/pki/etcd/`` directory.

In the following example, the tool prints the prefixes of API keys of etcd. 

````bash
./etcdctl --endpoints=https://127.0.0.1:2379 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key get / --prefix --keys-only 
````

The following command creates and saves the snapshot of etcd to a given file:

````bash
[root@kube-master1 etcd-v3.5.19-linux-amd64]# ./etcdctl --endpoints=https://127.0.0.1:2379 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key snapshot save /opt/etcd-backups/etcd-backup1
{"level":"info","ts":"2025-03-16T18:02:37.760533+0100","caller":"snapshot/v3_snapshot.go:65","msg":"created temporary db file","path":"/opt/etcd-backups/etcd-backup1.part"}
{"level":"info","ts":"2025-03-16T18:02:37.772177+0100","logger":"client","caller":"v3@v3.5.19/maintenance.go:212","msg":"opened snapshot stream; downloading"}
{"level":"info","ts":"2025-03-16T18:02:37.772292+0100","caller":"snapshot/v3_snapshot.go:73","msg":"fetching snapshot","endpoint":"https://127.0.0.1:2379"}
{"level":"info","ts":"2025-03-16T18:02:39.263730+0100","logger":"client","caller":"v3@v3.5.19/maintenance.go:220","msg":"completed snapshot read; closing"}
{"level":"info","ts":"2025-03-16T18:02:39.412698+0100","caller":"snapshot/v3_snapshot.go:88","msg":"fetched snapshot","endpoint":"https://127.0.0.1:2379","size":"75 MB","took":"1 second ago"}
{"level":"info","ts":"2025-03-16T18:02:39.412999+0100","caller":"snapshot/v3_snapshot.go:97","msg":"saved","path":"/opt/etcd-backups/etcd-backup1"}
Snapshot saved at /opt/etcd-backups/etcd-backup1
[root@kube-master1 etcd-v3.5.19-linux-amd64]# ll -h /opt/etcd-backups/etcd-backup1 
-rw-------. 1 root root 72M Mar 16 18:02 /opt/etcd-backups/etcd-backup1
[root@kube-master1 etcd-v3.5.19-linux-amd64]# 
````

The following command prints some useful information about the created backup:

````bash
[root@kube-master1 etcd-v3.5.19-linux-amd64]# ./etcdctl --write-out=table snapshot status /opt/etcd-backups/etcd-backup1 
Deprecated: Use `etcdutl snapshot status` instead.

+----------+----------+------------+------------+
|   HASH   | REVISION | TOTAL KEYS | TOTAL SIZE |
+----------+----------+------------+------------+
| fc20d4f8 |  1452263 |      25104 |      75 MB |
+----------+----------+------------+------------+
````

### Restoring Etcd from snapshot
The restore procedure looks like following:

- Stop core Kubernetes services. To do it, we shoul move the static pods' manifests to some other directory (Don't remove them). By doing this the core services will be disapper at the momment. Use ``crictl ps`` to verify that.

````bash
mkdir /etc/kubernetes/manifests_bckp
mv /etc/kubernetes/manifests/* /etc/kubernetes/manifests_bckp/
crictl ps
````

- Rename the etcd directory in ``/var/lib/etcd``.

Run restore command:

````bash
./etcdctl snapshot restore /opt/etcd-backups/etcd-backup1 /var/lib/etcd
````

- Move the static Pod files 

````bash
mv /etc/kubernetes/manifests_bckp/* /etc/kubernetes/manifests/
````

- check if the etcd runs again

````bash
crictl ps
kubectl get all
````

### Kubernetes Cluster Update

Kubernetes is able to upgrade from another minor version to anothe minor version. Skipping the version is not supported. For example, directly upgrade from 1.29 to 1.32 is not supported without upgrade to 1.30 and 1.31.

Here is the basic steps to proceed the upgrade:
- Upgrade kubeadm
- Update Kubernetes repository in ``/etc/yum.repos.d/kubernetes.repo``
- First are upgraded control nodes, second - worker nodes. It is not necessary to upgrade all immediately. You are able to run different versions some time.
- Use official documentation: https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/


- Here I updated the current version of the repository:

````bash
baseurl=https://pkgs.k8s.io/core:/stable:/v1.32/rpm/
````

#### kubeadm upgrade
Print the versions of all available versions of kubeadm:

````bash
sudo yum list --showduplicates kubeadm --disableexcludes=kubernetes
````

Installing the latest version which is available for now.

````bash
yum install kubeadm-'1.32.3-*' --disableexcludes=kubernetes
````

Verifying the current version

````bash
[root@kube-master1 ~]# kubeadm version
kubeadm version: &version.Info{Major:"1", Minor:"32", GitVersion:"v1.32.3", GitCommit:"32cc146f75aad04beaaa245a7157eb35063a9f99", GitTreeState:"clean", BuildDate:"2025-03-11T19:57:38Z", GoVersion:"go1.23.6", Compiler:"gc", Platform:"linux/amd64"}
````

#### Running upgrade of the control node

- Preparing upgrade plan:
````bash
kubeadm upgrade plan
````
- Running upgrade. It took for me max 5 minutes. 

````bash
kubeadm upgrade apply v1.32.3
.....

[upgrade] SUCCESS! A control plane node of your cluster was upgraded to "v1.32.3".
````

All these steps should be proceeded on each control node.

Once the upgrade of nodes finished, check from the admin host using ``kubectl`` the current version of the nodes.

````bash
seymur@fedora40:~$ kubectl get nodes
NAME                    STATUS   ROLES           AGE    VERSION
kube-master1.home.lab   Ready    control-plane   125d   v1.31.2
kube-master2.home.lab   Ready    control-plane   125d   v1.31.2
kube-master3.home.lab   Ready    control-plane   125d   v1.31.2
kube-worker1.home.lab   Ready    worker          125d   v1.31.2
kube-worker2.home.lab   Ready    worker          125d   v1.31.2
kube-worker3.home.lab   Ready    worker          125d   v1.31.2
````

It is still the previous version. In order to solve the issue, you should upgrade the kubelet and kubectl on the node VM.

#### Update kubelet and kubectl

Mark the node as unschedulable to set the node in maintenance

````bash
seymur@fedora40:~$ kubectl drain kube-master1.home.lab --ignore-daemonsets
````

Upgrade required packages

````bash
yum install -y kubelet-'1.32.3-*' kubectl-'1.32.3-*' --disableexcludes=kubernetes
````

Reload the kubelet service on the

````bash
systemctl daemon-reload
systemctl restart kubelet
````

Bring the node back online by marking it schedulable:

````bash
kubectl uncordon kube-master1.home.lab
````

#### Upgrade of worker nodes
Detailed documentation: https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/upgrading-linux-nodes/

1. Update the yum repo file
2. Update kubeadm

````bash
yum install -y kubeadm-'1.32.3-*' --disableexcludes=kubernetes
````

3. Upgrade node

````bash
kubeadm upgrade node
````

3. Drainin the host in order to set the node into maintenance

````bash
seymur@fedora40:~$ kubectl drain kube-worker1.home.lab --ignore-daemonsets 
````

4. Updating the kubelet and kubeadm

````bash
[root@kube-worker1 ~]# yum install -y kubelet-'1.32.3-*' kubectl-'1.32.3-*' --disableexcludes=kubernetes 
````

5. Reload daemon and restart kubelet

````bash
[root@kube-worker1 ~]# systemctl daemon-reload
[root@kube-worker1 ~]# systemctl restart kubelet
````

6. Uncording the worker node

````bash
seymur@fedora40:~$ kubectl uncordon kube-worker3.home.lab 
````



````bash
````
````bash
````
````bash
````
````bash
````
````bash
````
````bash
````
````bash
````
````bash
````



{% endraw %}

